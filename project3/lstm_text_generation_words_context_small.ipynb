{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sonnet 99 is not 14 lines, skipping\n",
      "Sonnet 126 is not 14 lines, skipping\n",
      "corpus length: 19972\n",
      "total words: 3160\n",
      "nb sequences: 19964\n",
      "Vectorization...\n",
      "Build model...\n"
     ]
    }
   ],
   "source": [
    "### https://github.com/keras-team/keras/blob/master/examples/lstm_text_generation.py\n",
    "\n",
    "''' Example script to generate text from Nietzsche's writings.\n",
    "\n",
    "At least 20 epochs are required before the generated text\n",
    "starts sounding coherent.\n",
    "\n",
    "It is recommended to run this script on GPU, as recurrent\n",
    "networks are quite computationally intensive.\n",
    "\n",
    "If you try this script on new data, make sure your corpus\n",
    "has at least ~100k characters. ~1M is better.\n",
    "'''\n",
    "\n",
    "from __future__ import print_function\n",
    "from keras.callbacks import LambdaCallback\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "from keras.layers import LSTM\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.utils.data_utils import get_file\n",
    "import numpy as np\n",
    "import random\n",
    "import sys\n",
    "import io\n",
    "\n",
    "from Sonnet_Set import Sonnet_Set\n",
    "from Sonnet_Set import Sequence_Type\n",
    "from Sonnet_Set import Element_Type\n",
    "\n",
    "sonnet_set = Sonnet_Set(\"data/shakespeare.txt\")\n",
    "sonnets = open(\"data/shakespeare.txt\")\n",
    "sonnet_sequences = sonnet_set.get_sequences(sequence_type=Sequence_Type.SONNET, element_type=Element_Type.WORD)\n",
    "\n",
    "text =[word for sonnet in sonnet_sequences for word in sonnet]\n",
    "print('corpus length:', len(text))\n",
    "\n",
    "words = sorted(list(set(text)))\n",
    "print('total words:', len(words))\n",
    "word_indices = dict((w, i) for i, w in enumerate(words))\n",
    "indices_word = dict((i, w) for i, w in enumerate(words))\n",
    "\n",
    "# cut the text in semi-redundant sequences of maxlen characters\n",
    "maxlen = 8\n",
    "step = 1\n",
    "sentences = []\n",
    "next_words = []\n",
    "for i in range(0, len(text) - maxlen, step):\n",
    "    sentences.append(text[i: i + maxlen])\n",
    "    next_words.append(text[i + maxlen])\n",
    "print('nb sequences:', len(sentences))\n",
    "\n",
    "print('Vectorization...')\n",
    "x = np.zeros((len(sentences), maxlen, len(words)), dtype=np.bool)\n",
    "y = np.zeros((len(sentences), len(words)), dtype=np.bool)\n",
    "for i, sentence in enumerate(sentences):\n",
    "    for t, word in enumerate(sentence):\n",
    "        x[i, t, word_indices[word]] = 1\n",
    "    y[i, word_indices[next_words[i]]] = 1\n",
    "\n",
    "\n",
    "# build the model: a single LSTM\n",
    "print('Build model...')\n",
    "model = Sequential()\n",
    "model.add(LSTM(10, input_shape=(maxlen, len(words))))\n",
    "model.add(Dense(len(words)))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "optimizer = RMSprop(lr=0.01)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer)\n",
    "\n",
    "\n",
    "def sample(preds, temperature=1.0):\n",
    "    # helper function to sample an index from a probability array\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/keras/models.py:944: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  warnings.warn('The `nb_epoch` argument in `fit` '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "19964/19964 [==============================] - 42s 2ms/step - loss: 6.2862\n",
      "Epoch 2/15\n",
      "19964/19964 [==============================] - 42s 2ms/step - loss: 5.8556\n",
      "Epoch 3/15\n",
      "19964/19964 [==============================] - 42s 2ms/step - loss: 5.6365\n",
      "Epoch 4/15\n",
      "19964/19964 [==============================] - 43s 2ms/step - loss: 5.5164\n",
      "Epoch 5/15\n",
      "19964/19964 [==============================] - 42s 2ms/step - loss: 5.4326\n",
      "Epoch 6/15\n",
      "19964/19964 [==============================] - 41s 2ms/step - loss: 5.3582\n",
      "Epoch 7/15\n",
      "19964/19964 [==============================] - 42s 2ms/step - loss: 5.2815\n",
      "Epoch 8/15\n",
      "19964/19964 [==============================] - 42s 2ms/step - loss: 5.2093\n",
      "Epoch 9/15\n",
      "19964/19964 [==============================] - 43s 2ms/step - loss: 5.1741\n",
      "Epoch 10/15\n",
      "19964/19964 [==============================] - 42s 2ms/step - loss: 5.1558\n",
      "Epoch 11/15\n",
      "19964/19964 [==============================] - 40s 2ms/step - loss: 5.1432\n",
      "Epoch 12/15\n",
      "19964/19964 [==============================] - 41s 2ms/step - loss: 5.1242\n",
      "Epoch 13/15\n",
      "19964/19964 [==============================] - 42s 2ms/step - loss: 5.0995\n",
      "Epoch 14/15\n",
      "19964/19964 [==============================] - 42s 2ms/step - loss: 5.0656\n",
      "Epoch 15/15\n",
      "19964/19964 [==============================] - 42s 2ms/step - loss: 5.0365\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7ff4d356e198>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x, y,\n",
    "         batch_size=128,\n",
    "         nb_epoch=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_sonnets = len(sonnet_sequences)\n",
    "num_words = len(words)\n",
    "        \n",
    "\n",
    "# Calculate sonnet context vectors\n",
    "\n",
    "sonnet_contexts = np.zeros((num_sonnets, num_words))\n",
    "\n",
    "for sonnet_index, sonnet in enumerate(sonnet_sequences):\n",
    "    for word in sonnet:\n",
    "        sonnet_contexts[sonnet_index, word] += 1\n",
    "\n",
    "row_sums = sonnet_contexts.sum(axis=1)\n",
    "sonnet_contexts = sonnet_contexts / row_sums[:, np.newaxis]\n",
    "\n",
    "column_means = sonnet_contexts.mean(axis=0)\n",
    "column_sds = sonnet_contexts.std(axis=0)\n",
    "\n",
    "sonnet_contexts = sonnet_contexts - column_means[np.newaxis, :]\n",
    "sonnet_contexts = sonnet_contexts / column_sds[np.newaxis, :]\n",
    "\n",
    "def calculate_word_sequence_context(word_sequence):\n",
    "    \n",
    "    word_sequence_context = np.zeros((num_words,))\n",
    "    \n",
    "    for word in word_sequence:\n",
    "        word_sequence_context[word] += 1\n",
    "    \n",
    "    word_sequence_context = word_sequence_context / sum(word_sequence_context)\n",
    "    \n",
    "    word_sequence_context = word_sequence_context - column_means\n",
    "    word_sequence_context = word_sequence_context / column_sds\n",
    "    \n",
    "    return word_sequence_context\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- diversity: 0.25\n",
      "----- Generating with seed -----\n",
      "I may not remove nor be removed:\n",
      ":\n",
      ":\n",
      ".\n",
      "----- End seed -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:76: RuntimeWarning: divide by zero encountered in log\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "And i i if i not to me be be,\n",
      "Or like by love's and and a time days place,\n",
      "To not to time with a his a strong see,\n",
      "So beauty's world shall i you to thee grace:\n",
      "To you to love with your and doth me me,\n",
      "O so all that i to your self love's thee,\n",
      "So in the love to my love to time be,\n",
      "When i which thou which in the love him be:\n",
      "To sad in and of my self but time time,\n",
      "In the love with with doth cold eyes to be,\n",
      "And in love to i love to be be time,\n",
      "But and a and a his youth to day be:\n",
      "  So say that with doth your love to be not,\n",
      "  So not i when with thy doth our doth not.\n",
      "----- diversity: 0.75\n",
      "----- Generating with seed -----\n",
      "I may not remove nor be removed:\n",
      ":\n",
      ":\n",
      ".\n",
      "----- End seed -----\n",
      "And in me looks to do i better dost hate,\n",
      "Time thou me then ill then not a so thing,\n",
      "That centre i a that eyed jewel love hate,\n",
      "Than fault night unworthiness fair best thing:\n",
      "Base not him to i nor your fair part denied,\n",
      "Interest dispraise in more such a hand might,\n",
      "So then jealous love chance saucy no beside,\n",
      "Near stay art sorrow how still trust as habitation might:\n",
      "Thy heart with make must gentle upon even,\n",
      "Term so surety-like she i me true writ,\n",
      "I art others hours and and i in heaven,\n",
      "That graces in my thy love with doth live writ:\n",
      "  Hard no leaves rage of very farther part,\n",
      "  Save thou set eyes as wakened thou enough heart.\n",
      "----- diversity: 1.5\n",
      "----- Generating with seed -----\n",
      "I may not remove nor be removed:\n",
      ":\n",
      ":\n",
      ".\n",
      "----- End seed -----\n",
      "And be nor to night that their worse hell bide,\n",
      "Leaves again for abuse it doth soul more,\n",
      "Time's filed thee perhaps vex remembered side,\n",
      "Losing should asleep truth proof dull grow'st abhor:\n",
      "Faces dear a hours forbid unfair rest see,\n",
      "Kind-hearted guides hopes enfeebled over-goes above,\n",
      "Nerves meadows accusing trial urge idolatry,\n",
      "Review thy to them inherit mountain love:\n",
      "Looking and your most perish commence be misplaced,\n",
      "Yet sensual heavenly first marvel honest moan,\n",
      "Voice shun despised middle what looks thou misplaced,\n",
      "Ripe eve's glory forbid the beds private upon:\n",
      "  Sway learn lover jacks untrue thine chronicle accident,\n",
      "  To send'st bounteous be beauty will thou accident.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "# Generating with some sensicalness\n",
    "\n",
    "random_sonnet_index = np.random.choice(range(len(sonnet_sequences)))\n",
    "random_sonnet = sonnet_sequences[random_sonnet_index]\n",
    "\n",
    "num_sonnets_to_compare_to = 20\n",
    "context_weight = 0.4\n",
    "\n",
    "# Pick the end of a line - this should serve as a decent seed for starting a new poem\n",
    "sentence = random_sonnet[-maxlen - 1:-1]\n",
    "\n",
    "start_index = random.randint(0, len(text) - maxlen - 1)\n",
    "for diversity in [0.25, 0.75, 1.5]:\n",
    "    print('----- diversity:', diversity)\n",
    "    \n",
    "    print(\"----- Generating with seed -----\")\n",
    "    sonnet_set.print_sonnet(sentence, sequence_type=Sequence_Type.SONNET, element_type=Element_Type.WORD)\n",
    "    print('----- End seed -----')\n",
    "    \n",
    "    current_phrase_window = sentence[:]\n",
    "    generated_sonnet = []\n",
    "    current_sonnet_line = 0\n",
    "    num_syllables_this_line = 0\n",
    "    previous_rhymable_words = [None, None]\n",
    "    \n",
    "    rhyming_words_vector = [1 if i in sonnet_set._rhyme_dictionary.keys() else 0 for i in range(len(words))]\n",
    "    not_new_line_vector = [1 if i < sonnet_set._word_dictionary[Sonnet_Set.NEW_LINE_CHARACTER]\n",
    "                           else 0 for i in range(len(words))]\n",
    "\n",
    "    while current_sonnet_line < 14:\n",
    "        \n",
    "        x_pred = np.zeros((1, maxlen, len(words)))\n",
    "        \n",
    "        for t, word in enumerate(current_phrase_window):\n",
    "            x_pred[0, t, word_indices[word]] = 1.\n",
    "        \n",
    "        preds = model.predict(x_pred, verbose=0)[0]\n",
    "        \n",
    "        if current_sonnet_line > 1:\n",
    "            word_sequence_context = calculate_word_sequence_context(generated_sonnet)\n",
    "            sonnet_similarities = np.zeros((num_sonnets,))\n",
    "            for sonnet_index, sonnet in enumerate(sonnet_sequences):\n",
    "                sonnet_similarity = np.matmul(word_sequence_context.T, sonnet_contexts[sonnet_index].T)\n",
    "                sonnet_similarities[sonnet_index] = sonnet_similarity\n",
    "            most_similar_sonnets = sonnet_similarities.argsort()[-num_sonnets_to_compare_to:][::-1]\n",
    "            least_similar_sonnets = sonnet_similarities.argsort()[0:num_sonnets_to_compare_to]\n",
    "            \n",
    "            context_weights = np.array((num_words,))\n",
    "            \n",
    "            for sonnet_index in most_similar_sonnets:\n",
    "                context_weights = context_weights + sonnet_contexts[sonnet_index]\n",
    "            \n",
    "            for sonnet_index in least_similar_sonnets:\n",
    "                context_weights = context_weights - sonnet_contexts[sonnet_index]\n",
    "            \n",
    "            context_weights = (context_weights - context_weights.min()) / context_weights.max()\n",
    "            \n",
    "            context_weights = context_weights / context_weights.sum()\n",
    "            \n",
    "            preds = (1 - context_weight) * preds + (context_weight * context_weights)\n",
    "        \n",
    "        # If we're on the last syllable, this must be a rhymable word\n",
    "        if num_syllables_this_line >= 9:\n",
    "            if current_sonnet_line in [0, 1, 4, 5, 8, 9, 12]:\n",
    "                preds = np.multiply(preds, rhyming_words_vector)\n",
    "                next_word = sample(preds, diversity)\n",
    "                if current_sonnet_line in [0, 4, 8, 12]:\n",
    "                    previous_rhymable_words[0] = next_word\n",
    "                else:\n",
    "                    previous_rhymable_words[1] = next_word\n",
    "            \n",
    "                #print(\"Next rhymable word is '%s'\" % sonnet_set._word_list[next_word])\n",
    "            elif current_sonnet_line in [2, 3, 6, 7, 10, 11, 13]:\n",
    "                \n",
    "                if current_sonnet_line in [2, 6, 10, 13]:\n",
    "                    previous_rhymable_word = previous_rhymable_words[0]\n",
    "                else:\n",
    "                    previous_rhymable_word = previous_rhymable_words[1]\n",
    "                    \n",
    "                rhyme_partners = sonnet_set._rhyme_pairs[sonnet_set._rhyme_dictionary[previous_rhymable_word]]\n",
    "                rhyme_partner_vector = [1 if i in rhyme_partners else 0 for i in range(len(preds))]\n",
    "                preds = np.multiply(preds, rhyme_partner_vector)\n",
    "                next_word = sample(preds, diversity)\n",
    "                previous_rhymable_word = None\n",
    "            \n",
    "                #print(\"Next rhyming word is '%s'\" % sonnet_set._word_list[next_word])\n",
    "            current_phrase_window = current_phrase_window[1:]+[next_word]\n",
    "            generated_sonnet.append(next_word)\n",
    "            num_syllables_this_line += sonnet_set._syllable_list_num[next_word][0]\n",
    "            next_word = sonnet_set._word_dictionary[Sonnet_Set.NEW_LINE_CHARACTER]\n",
    "            current_sonnet_line += 1\n",
    "            num_syllables_this_line = 0\n",
    "        else:\n",
    "            preds = np.multiply(preds, not_new_line_vector)\n",
    "            next_word = sample(preds, diversity)\n",
    "            num_syllables_this_line += sonnet_set._syllable_list_num[next_word][0]\n",
    "            \n",
    "            #print(\"Next word is '%s'\" % sonnet_set._word_list[next_word])\n",
    "            \n",
    "        current_phrase_window = current_phrase_window[1:]+[next_word]\n",
    "        generated_sonnet.append(next_word)\n",
    "        \n",
    "    #print(generated_sonnet)\n",
    "\n",
    "    sonnet_set.print_sonnet(generated_sonnet, sequence_type=Sequence_Type.SONNET, element_type=Element_Type.WORD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31\n"
     ]
    }
   ],
   "source": [
    "print(len(sonnet_set._character_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
