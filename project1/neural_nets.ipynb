{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.layers.core import Dense, Activation, Dropout\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_features, training_labels, validation_features, validation_labels = \\\n",
    "    utils.get_training_data(onehot=True, standardize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_11 (Dense)             (None, 160)               160160    \n",
      "_________________________________________________________________\n",
      "activation_11 (Activation)   (None, 160)               0         \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 160)               0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 2)                 322       \n",
      "_________________________________________________________________\n",
      "activation_12 (Activation)   (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 160,482\n",
      "Trainable params: 160,482\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 14000 samples, validate on 6000 samples\n",
      "Epoch 1/30\n",
      "14000/14000 [==============================] - 1s 73us/step - loss: 0.7182 - acc: 0.6571 - val_loss: 0.4027 - val_acc: 0.8255\n",
      "Epoch 2/30\n",
      "14000/14000 [==============================] - 1s 57us/step - loss: 0.4542 - acc: 0.7869 - val_loss: 0.3639 - val_acc: 0.8408\n",
      "Epoch 3/30\n",
      "14000/14000 [==============================] - 1s 57us/step - loss: 0.3888 - acc: 0.8275 - val_loss: 0.3514 - val_acc: 0.8442\n",
      "Epoch 4/30\n",
      "14000/14000 [==============================] - 1s 56us/step - loss: 0.3587 - acc: 0.8465 - val_loss: 0.3472 - val_acc: 0.8480\n",
      "Epoch 5/30\n",
      "14000/14000 [==============================] - 1s 59us/step - loss: 0.3425 - acc: 0.8516 - val_loss: 0.3464 - val_acc: 0.8485\n",
      "Epoch 6/30\n",
      "14000/14000 [==============================] - 1s 59us/step - loss: 0.3194 - acc: 0.8639 - val_loss: 0.3460 - val_acc: 0.8532\n",
      "Epoch 7/30\n",
      "14000/14000 [==============================] - 1s 57us/step - loss: 0.3110 - acc: 0.8674 - val_loss: 0.3497 - val_acc: 0.8525\n",
      "Epoch 8/30\n",
      "14000/14000 [==============================] - 1s 60us/step - loss: 0.2998 - acc: 0.8718 - val_loss: 0.3482 - val_acc: 0.8507\n",
      "Epoch 9/30\n",
      "14000/14000 [==============================] - 1s 56us/step - loss: 0.2884 - acc: 0.8782 - val_loss: 0.3504 - val_acc: 0.8503\n",
      "Epoch 10/30\n",
      "14000/14000 [==============================] - 1s 56us/step - loss: 0.2803 - acc: 0.8821 - val_loss: 0.3524 - val_acc: 0.8480\n",
      "Epoch 11/30\n",
      "14000/14000 [==============================] - 1s 57us/step - loss: 0.2740 - acc: 0.8886 - val_loss: 0.3538 - val_acc: 0.8495\n",
      "Epoch 12/30\n",
      "14000/14000 [==============================] - 1s 60us/step - loss: 0.2641 - acc: 0.8914 - val_loss: 0.3569 - val_acc: 0.8503\n",
      "Epoch 13/30\n",
      "14000/14000 [==============================] - 1s 56us/step - loss: 0.2576 - acc: 0.8952 - val_loss: 0.3590 - val_acc: 0.8503\n",
      "Epoch 14/30\n",
      "14000/14000 [==============================] - 1s 57us/step - loss: 0.2505 - acc: 0.8969 - val_loss: 0.3612 - val_acc: 0.8508\n",
      "Epoch 15/30\n",
      "14000/14000 [==============================] - 1s 57us/step - loss: 0.2426 - acc: 0.8986 - val_loss: 0.3644 - val_acc: 0.8510\n",
      "Epoch 16/30\n",
      "14000/14000 [==============================] - 1s 59us/step - loss: 0.2409 - acc: 0.9017 - val_loss: 0.3683 - val_acc: 0.8502\n",
      "Epoch 17/30\n",
      "14000/14000 [==============================] - 1s 60us/step - loss: 0.2319 - acc: 0.9037 - val_loss: 0.3696 - val_acc: 0.8498\n",
      "Epoch 18/30\n",
      "14000/14000 [==============================] - 1s 57us/step - loss: 0.2234 - acc: 0.9086 - val_loss: 0.3745 - val_acc: 0.8515\n",
      "Epoch 19/30\n",
      "14000/14000 [==============================] - 1s 56us/step - loss: 0.2253 - acc: 0.9066 - val_loss: 0.3763 - val_acc: 0.8515\n",
      "Epoch 20/30\n",
      "14000/14000 [==============================] - 1s 59us/step - loss: 0.2139 - acc: 0.9151 - val_loss: 0.3806 - val_acc: 0.8517\n",
      "Epoch 21/30\n",
      "14000/14000 [==============================] - 1s 57us/step - loss: 0.2086 - acc: 0.9164 - val_loss: 0.3815 - val_acc: 0.8517\n",
      "Epoch 22/30\n",
      "14000/14000 [==============================] - 1s 57us/step - loss: 0.2063 - acc: 0.9149 - val_loss: 0.3891 - val_acc: 0.8518\n",
      "Epoch 23/30\n",
      "14000/14000 [==============================] - 1s 62us/step - loss: 0.1957 - acc: 0.9209 - val_loss: 0.3920 - val_acc: 0.8493\n",
      "Epoch 24/30\n",
      "14000/14000 [==============================] - 1s 57us/step - loss: 0.1912 - acc: 0.9239 - val_loss: 0.3977 - val_acc: 0.8492\n",
      "Epoch 25/30\n",
      "14000/14000 [==============================] - 1s 57us/step - loss: 0.1855 - acc: 0.9273 - val_loss: 0.4007 - val_acc: 0.8518\n",
      "Epoch 26/30\n",
      "14000/14000 [==============================] - 1s 57us/step - loss: 0.1841 - acc: 0.9277 - val_loss: 0.4028 - val_acc: 0.8495\n",
      "Epoch 27/30\n",
      "14000/14000 [==============================] - 1s 62us/step - loss: 0.1800 - acc: 0.9286 - val_loss: 0.4077 - val_acc: 0.8503\n",
      "Epoch 28/30\n",
      "14000/14000 [==============================] - 1s 57us/step - loss: 0.1714 - acc: 0.9332 - val_loss: 0.4149 - val_acc: 0.8507\n",
      "Epoch 29/30\n",
      "14000/14000 [==============================] - 1s 56us/step - loss: 0.1712 - acc: 0.9336 - val_loss: 0.4132 - val_acc: 0.8508\n",
      "Epoch 30/30\n",
      "14000/14000 [==============================] - 1s 54us/step - loss: 0.1688 - acc: 0.9358 - val_loss: 0.4186 - val_acc: 0.8503\n",
      "Model 1 test score: 0.418589910586675\n",
      "Model 1 test accuracy: 0.8503333333333334\n"
     ]
    }
   ],
   "source": [
    "# Our first 160-unit model\n",
    "model1 = Sequential()\n",
    "\n",
    "# A single fully connected layer with 160 units\n",
    "model1.add(Dense(160, input_shape=(training_features.shape[1],)))\n",
    "model1.add(Activation('sigmoid'))\n",
    "\n",
    "# Drop out 10% of units for regularization\n",
    "model1.add(Dropout(0.75))\n",
    "\n",
    "# Softmax classification layer\n",
    "model1.add(Dense(2))\n",
    "model1.add(Activation('softmax'))\n",
    "\n",
    "## Printing a summary of the layers and weights in your model\n",
    "model1.summary()\n",
    "\n",
    "model1.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "fit = model1.fit(training_features, training_labels, batch_size=256, epochs=30, verbose=1,\n",
    "                 validation_data=(validation_features, validation_labels))\n",
    "\n",
    "score1 = model1.evaluate(validation_features, validation_labels, verbose=0)\n",
    "print('Model 1 test score:', score1[0])\n",
    "print('Model 1 test accuracy:', score1[1])\n",
    "\n",
    "best_score = score1\n",
    "best_model = model1\n",
    "best_num_epochs = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's crank up the layers\n",
    "model2 = Sequential()\n",
    "\n",
    "# The first fully connected layer with 160 units\n",
    "model2.add(Dense(160, input_shape=(training_features.shape[1],)))\n",
    "model2.add(Activation('sigmoid'))\n",
    "\n",
    "# Drop out 30% of units for regularization\n",
    "model2.add(Dropout(0.75))\n",
    "\n",
    "# A second fully connected layer with 60 units\n",
    "model2.add(Dense(60, input_shape=(training_features.shape[1],)))\n",
    "model2.add(Activation('sigmoid'))\n",
    "\n",
    "# Drop out 10% of units for regularization\n",
    "model2.add(Dropout(0.75))\n",
    "\n",
    "# Softmax classification layer\n",
    "model2.add(Dense(2))\n",
    "model2.add(Activation('softmax'))\n",
    "\n",
    "## Printing a summary of the layers and weights in your model\n",
    "model2.summary()\n",
    "\n",
    "model2.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "\n",
    "fit = model2.fit(training_features, training_labels, batch_size=256, epochs=30, verbose=1,\n",
    "                 validation_data=(validation_features, validation_labels))\n",
    "\n",
    "score2 = model2.evaluate(validation_features, validation_labels, verbose=0)\n",
    "print('Model 2 test score:', score2[0])\n",
    "print('Model 2 test accuracy:', score2[1])\n",
    "\n",
    "if score2 > best_score:\n",
    "    best_score = score2\n",
    "    best_model = model2\n",
    "    best_num_epochs = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Time for 3 layers!\n",
    "model3 = Sequential()\n",
    "\n",
    "# The first fully connected layer with 140 units\n",
    "model3.add(Dense(140, input_shape=(training_features.shape[1],)))\n",
    "model3.add(BatchNormalization())\n",
    "model3.add(Activation('sigmoid'))\n",
    "\n",
    "# Drop out 40% of units for regularization\n",
    "model3.add(Dropout(0.75))\n",
    "\n",
    "# A second fully connected layer with 60 units\n",
    "model3.add(Dense(60, input_shape=(training_features.shape[1],)))\n",
    "model3.add(BatchNormalization())\n",
    "model3.add(Activation('sigmoid'))\n",
    "\n",
    "# Drop out 25% of units for regularization\n",
    "model3.add(Dropout(0.75))\n",
    "\n",
    "# A third fully connected layer with 60 units\n",
    "model3.add(Dense(25, input_shape=(training_features.shape[1],)))\n",
    "model3.add(BatchNormalization())\n",
    "model3.add(Activation('sigmoid'))\n",
    "\n",
    "# Drop out 10% of units for regularization\n",
    "model3.add(Dropout(0.50))\n",
    "\n",
    "# Softmax classification layer\n",
    "model3.add(Dense(2))\n",
    "model3.add(Activation('softmax'))\n",
    "\n",
    "## Printing a summary of the layers and weights in your model\n",
    "model3.summary()\n",
    "\n",
    "model3.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "\n",
    "fit = model3.fit(training_features, training_labels, batch_size=256, epochs=100, verbose=1,\n",
    "                 validation_data=(validation_features, validation_labels))\n",
    "\n",
    "score3 = model3.evaluate(validation_features, validation_labels, verbose=0)\n",
    "print('Model 3 test score:', score2[0])\n",
    "print('Model 3 test accuracy:', score2[1])\n",
    "\n",
    "if score3 > best_score:\n",
    "    best_score = score3\n",
    "    best_model = model3\n",
    "    best_num_epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score1 = model1.evaluate(validation_features, validation_labels, verbose=0)\n",
    "print('Model 1 test score:', score1[0])\n",
    "print('Model 1 test accuracy:', score1[1])\n",
    "\n",
    "score2 = model2.evaluate(validation_features, validation_labels, verbose=0)\n",
    "print('Model 2 test score:', score2[0])\n",
    "print('Model 2 test accuracy:', score2[1])\n",
    "\n",
    "score3 = model3.evaluate(validation_features, validation_labels, verbose=0)\n",
    "print('Model 3 test score:', score3[0])\n",
    "print('Model 3 test accuracy:', score3[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now the whole dataset with the best model\n",
    "\n",
    "training_features, training_labels, _, _ = utils.get_training_data(onehot=True, standardize=True, validation_size=0.0)\n",
    "\n",
    "best_model.fit(training_features, training_labels, batch_size=256, epochs=best_num_epochs, verbose=1)\n",
    "\n",
    "test_features = utils.get_testing_data(standardize=True)\n",
    "\n",
    "test_labels = best_model.predict(test_features)\n",
    "\n",
    "utils.save_prediction(test_labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
